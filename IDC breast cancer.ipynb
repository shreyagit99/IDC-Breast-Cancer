{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nIMAGE_SIZE = 96\nIMAGE_CHANNELS = 3\n\nSAMPLE_SIZE = 80000 \n","metadata":{"_uuid":"b7bbdd52c81188b8e9c528b88d9fd0da176bf4bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What files are available?","metadata":{"_uuid":"25802a26e4afba8f6dc42754f7f61da4c8cfea5c"}},{"cell_type":"code","source":"os.listdir('../input/histopathologic-cancer-detection')","metadata":{"_uuid":"699bb899bde433ba20fb0d086fa0f33a0f61a250","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Labels as per csv file\n\n0 = no met tissue<br>\n1 =   has met tissue. <br>\n","metadata":{"_uuid":"c24d1662f8bd5fc4d8c57c29449990a3520cd96b"}},{"cell_type":"markdown","source":"### How many training images are in each folder?","metadata":{"_uuid":"5a285343c286be191aa827ff9b836b67821176a5"}},{"cell_type":"code","source":"\nprint(len(os.listdir('../input/histopathologic-cancer-detection/train')))\n","metadata":{"_uuid":"54461212efed65ac377369a468c80e7d708010f4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Dataframe containing all images","metadata":{"_uuid":"b90854e07d495d9f945a0e40189fd32a0c32bff5"}},{"cell_type":"code","source":"df_data = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\n# removing an erroneous image\ndf_data = df_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing an erroneous image\ndf_data = df_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n\n\nprint(df_data.shape)","metadata":{"_uuid":"e9c9f40ffab35044641b0dc7d9b18609af1aa25e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the class distribution","metadata":{"_uuid":"cfbd53b7f8ea1929952ffed6221b380012618e32"}},{"cell_type":"code","source":"df_data['label'].value_counts()","metadata":{"_uuid":"e18560bf69d3dfc0c4772e7c79bb119fd2eb634b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display a random sample of train images  by class","metadata":{"_uuid":"6efe2e5de99c4bf92079b1a7d0b892d30fc9d518"}},{"cell_type":"code","source":"# source: https://www.kaggle.com/gpreda/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) \n    \n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols)\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"_kg_hide-input":true,"_uuid":"1c5143f227da4262eafce8cf0210a02c8072fb8e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_PATH = '../input/histopathologic-cancer-detection/train/' \n\ndraw_category_images('label',4, df_data, IMAGE_PATH)","metadata":{"_uuid":"bd38bcfb5839975e4fee9e70b93d42c29c1b5d2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the Train and Val Sets","metadata":{"_uuid":"1da4226777aefe65b1bb3430208ea91ea7ca7d9a"}},{"cell_type":"code","source":"df_data.head()","metadata":{"_uuid":"547f9f571ee82e20b7647e16ed36de7550046032","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Balance the target distribution\nWe will reduce the number of samples in class 0.","metadata":{"_uuid":"c1150500d2772b7f36cdaa5aa5fd7f0fb4a72628"}},{"cell_type":"code","source":"\ndf_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n\ndf_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n\n\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","metadata":{"_uuid":"270fc18640b552ecc3cb0e1dd3036441db7a4a2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head()","metadata":{"_uuid":"a166dec3ef84c66ad9cd815b63fc1a753df2eb76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ny = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","metadata":{"_uuid":"15ba9792e6a370b7560330af15b3cfe21185c1cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['label'].value_counts()","metadata":{"_uuid":"7de70d915a5f1d2599725e00bdb3b9103d947883","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['label'].value_counts()","metadata":{"_uuid":"392c0eea00be8e43a6e55438d1458650e842030b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Directory Structure","metadata":{"_uuid":"ba17dd34b75367fc61df6634d51dac94c3ab4951"}},{"cell_type":"code","source":"\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n\n\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n\n\nno_met_tissue = os.path.join(train_dir, 'a_no_met_tissue')\nos.mkdir(no_met_tissue)\nhas_met_tissue = os.path.join(train_dir, 'b_has_met_tissue')\nos.mkdir(has_met_tissue)\n\n\n\nno_met_tissue = os.path.join(val_dir, 'a_no_met_tissue')\nos.mkdir(no_met_tissue)\nhas_met_tissue = os.path.join(val_dir, 'b_has_met_tissue')\nos.mkdir(has_met_tissue)\n\n","metadata":{"_uuid":"ff8acc2e92a1b1b5002d6e1bf9a1180c3256f19d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nos.listdir('base_dir/train_dir')","metadata":{"_uuid":"03ca5d4b8b027c2712d7096314d3a79ef829b23c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transfer the images into the folders","metadata":{"_uuid":"6a2e56340ba18f3b63c1b129fd995fecfadaa21d"}},{"cell_type":"code","source":"\ndf_data.set_index('id', inplace=True)","metadata":{"_uuid":"e84c8a9642b030094b1888af3299063f883112a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])\n\n\n\n\n\nfor image in train_list:\n\n    fname_tif = image + '.tif'\n\n    target = df_data.loc[image,'label']\n    \n   \n    if target == 0:\n        label = 'a_no_met_tissue'\n    if target == 1:\n        label = 'b_has_met_tissue'\n    \n\n    src = os.path.join('../input/histopathologic-cancer-detection/train', fname_tif)\n   \n    fname_png = image + '.png'\n \n    dst = os.path.join(train_dir, label, fname_png)\n\n    \n   \n    cv2_image = cv2.imread(src)\n   \n    cv2.imwrite(dst, cv2_image)\n \n\n\n\nfor image in val_list:\n    \n  \n    fname_tif = image + '.tif'\n  \n    target = df_data.loc[image,'label']\n\n    if target == 0:\n        label = 'a_no_met_tissue'\n    if target == 1:\n        label = 'b_has_met_tissue'\n    \n    src = os.path.join('../input/histopathologic-cancer-detection/train', fname_tif)\n\n    fname_png = image + '.png'\n \n    dst = os.path.join(val_dir, label, fname_png)\n\n    \n\n    cv2_image = cv2.imread(src)\n    \n    cv2.imwrite(dst, cv2_image)\n\n\n   ","metadata":{"_uuid":"afb8969a9ee75c13bddc808a4bcc326611baaaaf","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint(len(os.listdir('base_dir/train_dir/a_no_met_tissue')))\nprint(len(os.listdir('base_dir/train_dir/b_has_met_tissue')))\n","metadata":{"_uuid":"71532bfc32608289b1f773ffdbc8a7cea1bfb94c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint(len(os.listdir('base_dir/val_dir/a_no_met_tissue')))\nprint(len(os.listdir('base_dir/val_dir/b_has_met_tissue')))\n","metadata":{"_uuid":"897e9df543bb65b47bb00019dc681125ca08ee5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ef780ac9a9fc89b4ff4f042593eb68992f354a1d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set Up the Generators","metadata":{"_uuid":"f8dce940ee8a7a42aacb062e4c6b5a4a54dba58f"}},{"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\ntest_path = '../input/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","metadata":{"_uuid":"ef4fe7be09f11ff4badfd22d5fd5e03f8521ed58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","metadata":{"_uuid":"68fbd9d5fbb80859a82f94a12e335ce05a93bd51","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the Model Architecture¶","metadata":{"_uuid":"79da4d0a66a90cffe40580a596dd4d0e2bc45a9b"}},{"cell_type":"code","source":"\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()\n","metadata":{"_uuid":"b9835ea0fd0bca54138904895c39d38227a70c22","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{"_uuid":"75cfc4fcb8dd3408d1c4fcf8cd85e0e2f5b611d7"}},{"cell_type":"code","source":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","metadata":{"_uuid":"9de9715f49a63b55775b10abd2f461b395e23b5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(val_gen.class_indices)","metadata":{"_uuid":"227d84a4f44c0b7256855c06ba04dabd58d89d84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=20, verbose=1,\n                   callbacks=callbacks_list)","metadata":{"scrolled":true,"_uuid":"a746769db61563f226288eba9aa8a6584b9e8e0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate the model using the val set","metadata":{"_uuid":"fa15a8afda3593973726e9087cbd98073041c908"}},{"cell_type":"code","source":"\nmodel.metrics_names","metadata":{"_uuid":"70104420ec7f400cd06203f875dbeba30f4d8a96","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","metadata":{"_uuid":"428bdf5b24ff8cef35012205c3f2eb37006fc9e9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the Training Curves","metadata":{"_uuid":"93556c9e4b6a188cf9cb67a6519c9bc365c60caf"}},{"cell_type":"code","source":"\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss', color='red')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, label='Training acc')\nplt.plot(epochs, val_acc, label='Validation acc', color='red')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","metadata":{"_uuid":"385da8ba94a1079d17909790716b295fc2737584","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a prediction on the val set\nWe need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score.","metadata":{"_uuid":"5636e76f23202dd1f2a27ace25e15e09619a5e4e"}},{"cell_type":"code","source":"\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","metadata":{"_uuid":"652d9d6aa51dc1818d1c5171212d10e141ad7de9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"_uuid":"edf1866df4638ded26de2e0e3d2ba0f5e00e1ace","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A note on Keras class index values**\n\nKeras assigns it's own index value (here 0 and 1) to the classes. It infers the classes based on the folder structure.\nImportant: These index values may not match the index values we were given in the train_labels.csv file.\n\nI've used 'a' and 'b' folder name pre-fixes to get keras to assign index values to match what was in the train_labels.csv file - I guessed that keras is assigning the index value based on folder name alphabetical order.","metadata":{"_uuid":"6bbf4f095fe5412561924c09c96d8dc2ea4adfc0"}},{"cell_type":"code","source":"\ntest_gen.class_indices","metadata":{"_uuid":"dc71f69944e7db83329417c5265a5bc31f9c4fc3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndf_preds = pd.DataFrame(predictions, columns=['no_met_tissue', 'has_met_tissue'])\n\ndf_preds.head()","metadata":{"_uuid":"4a6709d73969f7fd597128223b110be077f84edb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_true = test_gen.classes\n\n\ny_pred = df_preds['has_met_tissue']\n","metadata":{"_uuid":"0954d61a4ef8bc056452b3bbad9456d45c00bed1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the AUC Score?","metadata":{"_uuid":"f07c814d213e814cdac4e3d05d0a8db847fbfe28"}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_true, y_pred)","metadata":{"_uuid":"0b7b7a56c6fa47cc40764d0c06d64860580cbea1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Confusion Matrix","metadata":{"_uuid":"9d8404e8e6b6008c8989b8184700ec5562b99366"}},{"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"_uuid":"91f570e8e5f07126e5361bbf92929d786e853a09","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntest_labels = test_gen.classes","metadata":{"_uuid":"c20bc358dc753020d5a560dc56f2350c7f40a4f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels.shape","metadata":{"_uuid":"3a537724473df19c74bb1bf6928f531dd0fcfdb3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","metadata":{"_uuid":"0bb4323e931ff53081994bbf58e82b1ec93ab327","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_gen.class_indices","metadata":{"_uuid":"ec2058223eb30485898a12c0e904bb170c0aa884","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncm_plot_labels = ['no_met_tissue', 'has_met_tissue']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","metadata":{"_uuid":"ba26c7e718df937a18aa2035c4ba883252e44c79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Classification Report","metadata":{"_uuid":"2576c1144cfe93d66f3197396990f3e43addd499"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred_binary = predictions.argmax(axis=1)\n\nreport = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n\nprint(report)\n","metadata":{"_uuid":"91dcace7eb99aca310774b7a3a55535c9127ce55","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision **= Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n\nFrom the confusion matrix and classification report we see that our model is equally good at detecting both classes.","metadata":{"_uuid":"36880545abb112946ebe48242d5da6c8517cf61b"}}]}